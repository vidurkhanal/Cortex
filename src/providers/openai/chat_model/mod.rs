use crate::{
    model::{
        LanguageModel, LanguageModelCall, LanguageModelDoGenerateRequest,
        LanguageModelDoGenerateResponse,
    },
    providers::openai::ModelError,
};
pub mod model_id;

pub enum OpenAIChatSettingsReasoningEffort {
    /// No reasoning effort, the model will not perform any reasoning.
    None,

    /// Low reasoning effort, the model will perform minimal reasoning.
    Low,

    /// Medium reasoning effort, the model will perform moderate reasoning.
    Medium,

    /// High reasoning effort, the model will perform extensive reasoning.
    High,
}

pub struct OpenAIChatModel {
    /// Modify the likelihood of specified tokens appearing in the completion.
    ///
    /// Accepts a JSON object that maps tokens (specified by their token ID in
    /// the GPT tokenizer) to an associated bias value from -100 to 100. You
    /// the bias is added to the logits generated by the model prior to sampling.
    /// The exact effect will vary per model, but values between -1 and 1 shoul
    /// }
    ///
    /// decrease or increase likelihood of selection; values like -100 or 100
    /// should result in a ban or exclusive selection of the relevant token.
    ///
    /// As an example, you can pass {"50256": -100} to prevent the <|endoftext|>
    /// token from being generated.
    pub logit_bias: Option<Vec<(String, f32)>>,
    /// Return the log probabilities of the tokens. Including logprobs will increase
    /// the response size and can slow down response times. However, it can
    /// be useful to better understand how the model is behaving.
    ///
    /// Setting to true will return the log probabilities of the tokens that
    /// were generated.
    ///
    /// Setting to a number will return the log probabilities of the top n
    /// tokens that were generated.
    pub log_probs: Option<u32>,
    /// Whether to enable parallel function calling during tool use. Default to true.
    pub parallel_calls: bool,
    /// Whether to use structured outputs. Defaults to false.
    ///
    /// When enabled, tool calls and object generation will be strict and follow the provided schema.
    pub structured_output: bool,
    /// A unique identifier representing your end-user, which can help OpenAI to
    /// monitor and detect abuse. Learn more.
    pub user: Option<String>,
    /// Automatically download images and pass the image as data to the model.
    /// OpenAI supports image URLs for public models, so this is only needed for
    /// private models or when the images are not publicly accessible.
    ///
    /// Defaults to `false`.
    pub download_images: bool,
    /// Reasoning effort for reasoning models. Defaults to `medium`.
    pub reasoning_effort: OpenAIChatSettingsReasoningEffort,
}

impl Default for OpenAIChatModel {
    fn default() -> Self {
        OpenAIChatModel {
            logit_bias: None,
            log_probs: None,
            parallel_calls: true,
            structured_output: false,
            user: None,
            download_images: false,
            reasoning_effort: OpenAIChatSettingsReasoningEffort::Medium,
        }
    }
}

impl OpenAIChatModel {
    pub fn new() -> Self {
        OpenAIChatModel::default()
    }

    pub fn generate() -> Result<(), ModelError> {
        Err(ModelError::NotSupported(
            "OpenAIChatModel does not support generate operation".to_string(),
        ))
    }

    pub fn with_logit_bias(mut self, logit_bias: Vec<(String, f32)>) -> Self {
        self.logit_bias = Some(logit_bias);
        self
    }

    pub fn with_log_probs(mut self, log_probs: u32) -> Self {
        self.log_probs = Some(log_probs);
        self
    }

    pub fn with_parallel_calls(mut self, parallel_calls: bool) -> Self {
        self.parallel_calls = parallel_calls;
        self
    }

    pub fn with_structured_output(mut self, structured_output: bool) -> Self {
        self.structured_output = structured_output;
        self
    }

    pub fn with_user(mut self, user: String) -> Self {
        self.user = Some(user);
        self
    }

    pub fn with_download_images(mut self, download_images: bool) -> Self {
        self.download_images = download_images;
        self
    }

    pub fn with_reasoning_effort(
        mut self,
        reasoning_effort: OpenAIChatSettingsReasoningEffort,
    ) -> Self {
        self.reasoning_effort = reasoning_effort;
        self
    }
}

impl LanguageModel for OpenAIChatModel {
    fn do_generate(
        &self,
        model_call: LanguageModelDoGenerateRequest,
    ) -> Result<LanguageModelDoGenerateResponse, ModelError> {
        todo!()
    }

    fn supports_urls(&self, url: String) -> bool {
        todo!()
    }
}
